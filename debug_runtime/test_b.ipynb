{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "291b38f07e55"
      },
      "source": [
        "# Part B (stadlone if wanted) â€” verify PyTorch ROCm after install\n",
        "\n",
        "Run this notebook after youve run the install notebook (Part A) and restarted the kernel / started a fresh run.\n",
        "\n",
        "It prints:\n",
        "- the kernel Python (`sys.executable`)\n",
        "- where `torch` is imported from\n",
        "- rocm/hiop version info\n",
        "- number of GPUs and their names\n",
        "- a tiny GPU matmul test\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a84c4531b44"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import sys, platform\n",
        "print('=== KERNEL PYTHON ===')\n",
        "print('sys.executable:', sys.executable)\n",
        "print('sys.version:', sys.version)\n",
        "print('platform:', platform.platform())\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97cca6325158"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import torch\n",
        "print('\\n=== TORCH BUILD ===')\n",
        "print('torch.__version__:', torch.__version__)\n",
        "print('torch file:', torch.__file__)\n",
        "print('torch.version.hip:', getattr(torch.version, 'hip', None))\n",
        "print('torch.version.cuda:', getattr(torch.version, 'cuda', None))\n",
        "print('torch.cuda.is_available():', torch.cuda.is_available())\n",
        "print('torch.cuda.device_count():', torch.cuda.device_count())\n",
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f'Device {i}:', torch.cuda.get_device_name(i))\n",
        "else:\n",
        "    print('No torch-visible GPU.')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caa4cedaf371"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import time, torch\n",
        "print('\\n=== TINY GPU TEST (matmul on cuda:0) ===')\n",
        "assert torch.cuda.is_available(), 'No torch-visible GPU.'\n",
        "device = torch.device('cuda:0')\n",
        "a = torch.randn((2048, 2048), device=device, dtype=torch.float16)\n",
        "b = torch.randn((2048, 2048), device=device, dtype=torch.float16)\n",
        "for _ in range(5):\n",
        "    c = a @ b\n",
        "torch.cuda.synchronize(device)\n",
        "t0 = time.time()\n",
        "for _ in range(20):\n",
        "    c = a @ b\n",
        "torch.cuda.synchronize(device)\n",
        "t1 = time.time()\n",
        "print('OK | elapsed:', round(t1-t0, 6), 's | mean:', float(c.mean().item()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99d1de3bb1dc"
      },
      "source": [
        "## Optional: per-GPU matmul (stress each GPU,might be nice to see it altogehter)\n",
        "Uncomment and run if you want to verify compute on every GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b9765bb9cae"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# import time, torch\n",
        "# assert torch.cuda.is_available(), 'No torch-visible GPU.'\n",
        "# \n",
        "# def matmul_on_device(i: int, n: int = 2048, iters: int = 20, warmup: int = 5):\n",
        "#     device = torch.device(f'cuda:{i}')\n",
        "#     a = torch.randn((n, n), device=device, dtype=torch.float16)\n",
        "#     b = torch.randn((n, n), device=device, dtype=torch.float16)\n",
        "#     for _ in range(warmup):\n",
        "#         c = a @ b\n",
        "#     torch.cuda.synchronize(device)\n",
        "#     t0 = time.time()\n",
        "#     for _ in range(iters):\n",
        "#         c = a @ b\n",
        "#     torch.cuda.synchronize(device)\n",
        "#     t1 = time.time()\n",
        "#     return (t1 - t0), float(c.mean().item())\n",
        "# \n",
        "# count = torch.cuda.device_count()\n",
        "# print(f'Running per-GPU matmul test on {count} GPU(s)...')\n",
        "# for i in range(count):\n",
        "#     elapsed, meanv = matmul_on_device(i)\n",
        "#     print(f' GPU {i}: OK | elapsed={elapsed:.4f}s | mean={meanv:.6f}')\n"
      ]
    }
  ]
}
