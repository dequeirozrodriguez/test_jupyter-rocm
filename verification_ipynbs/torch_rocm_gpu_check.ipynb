{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Torch / ROCm GPU sanity check!\n",
        "\n",
        "thhis notebook prints Pytorch plus ROCm info and runs a tiny GPU matmul if a GPU is visible.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, platform\n",
        "print('Python:', sys.version)\n",
        "print('Platform:', platform.platform())\n",
        "print('Executable:', sys.executable)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "print('torch.__version__:', torch.__version__)\n",
        "print('torch.version.hip:', getattr(torch.version, 'hip', None))\n",
        "print('torch.version.cuda:', getattr(torch.version, 'cuda', None))\n",
        "print('torch.cuda.is_available():', torch.cuda.is_available())\n",
        "print('torch.cuda.device_count():', torch.cuda.device_count())\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f'Device {i}:', torch.cuda.get_device_name(i))\n",
        "        props = torch.cuda.get_device_properties(i)\n",
        "        # Some properties may not exist on all builds; keep it robust\n",
        "        print('  total_memory (GiB):', getattr(props, 'total_memory', 0)/1024**3)\n",
        "        print('  multi_processor_count:', getattr(props, 'multi_processor_count', None))\n",
        "        print('  gcnArchName:', getattr(props, 'gcnArchName', None))\n",
        "else:\n",
        "    print('\\nNo GPU visible to torch')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "\n",
        "def run_matmul(device):\n",
        "    # some smallish matmul to confirm compute :)\n",
        "    a = torch.randn((2048, 2048), device=device, dtype=torch.float16)\n",
        "    b = torch.randn((2048, 2048), device=device, dtype=torch.float16)\n",
        "    # warmup\n",
        "    for _ in range(5):\n",
        "        c = a @ b\n",
        "    torch.cuda.synchronize() if device.type == 'cuda' else None\n",
        "    t0 = time.time()\n",
        "    for _ in range(20):\n",
        "        c = a @ b\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "    t1 = time.time()\n",
        "    print(f'Matmul OK on {device} | elapsed: {t1 - t0:.3f}s | c.mean(): {c.mean().item():.6f}')\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    run_matmul(torch.device('cuda:0'))\n",
        "else:\n",
        "    print('Skipping matmul: no torch-visible GPU.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess, shlex\n",
        "\n",
        "def try_cmd(cmd):\n",
        "    print('\\n$ ' + cmd)\n",
        "    try:\n",
        "        out = subprocess.check_output(shlex.split(cmd), stderr=subprocess.STDOUT, text=True)\n",
        "        print(out[:4000])\n",
        "        if len(out) > 4000:\n",
        "            print('... (truncated)')\n",
        "    except FileNotFoundError:\n",
        "        print('Command not found.')\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print('Command failed with code', e.returncode)\n",
        "        print((e.output or '')[:4000])\n",
        "\n",
        "try_cmd('rocminfo')\n",
        "try_cmd('rocm-smi')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}