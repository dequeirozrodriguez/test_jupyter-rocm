{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66543d312319"
      },
      "source": [
        "# MultigPU Torch sanity check (ROCm)\n",
        "\n",
        "This notebook does the following for us 9if everything is indeed fine):\n",
        "- prints PyTorch build info\n",
        "- prints how many GPUs torch sees\n",
        "- runs a small matmul on **each** GPU and prints `OK` per device :)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28afeacd0efc"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import sys, platform\n",
        "print('Python:', sys.version)\n",
        "print('Platform:', platform.platform())\n",
        "print('Executable:', sys.executable)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "521a408d5c03"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import torch\n",
        "print('torch.__version__:', torch.__version__)\n",
        "print('torch.version.hip:', getattr(torch.version, 'hip', None))\n",
        "print('torch.cuda.is_available():', torch.cuda.is_available())\n",
        "print('torch.cuda.device_count():', torch.cuda.device_count())\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f'Device {i}:', torch.cuda.get_device_name(i))\n",
        "else:\n",
        "    print('No GPU visible to torch.')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c10dba63edd"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "\n",
        "def matmul_on_device(i: int, n: int = 2048, iters: int = 20, warmup: int = 5):\n",
        "    device = torch.device(f'cuda:{i}')\n",
        "    # just for the sake of it, use fp16 like typical ROCm workloads; reduce n if you want it faster.\n",
        "    a = torch.randn((n, n), device=device, dtype=torch.float16)\n",
        "    b = torch.randn((n, n), device=device, dtype=torch.float16)\n",
        "    # warmup just to see the usage u know\n",
        "    for _ in range(warmup):\n",
        "        c = a @ b\n",
        "    torch.cuda.synchronize(device)\n",
        "    t0 = time.time()\n",
        "    for _ in range(iters):\n",
        "        c = a @ b\n",
        "    torch.cuda.synchronize(device)\n",
        "    t1 = time.time()\n",
        "    # touch result so it cantt be optimized away; mean triggers a reduction so an opps.\n",
        "    m = c.mean().item()\n",
        "    return (t1 - t0), m\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    raise SystemExit('No torch-visible GPU; nothing to test ;((.')\n",
        "\n",
        "count = torch.cuda.device_count()\n",
        "print(f'Running matmul test on {count} GPU(s)...')\n",
        "\n",
        "all_ok = True\n",
        "results = []\n",
        "for i in range(count):\n",
        "    try:\n",
        "        elapsed, meanv = matmul_on_device(i)\n",
        "        print(f'GPU {i}: OK | elapsed={elapsed:.4f}s | mean={meanv:.6f}')\n",
        "        results.append((i, True, elapsed, meanv))\n",
        "    except Exception as e:\n",
        "        all_ok = False\n",
        "        print(f'GPU {i}: FAIL | {type(e).__name__}: {e}')\n",
        "        results.append((i, False, None, None))\n",
        "\n",
        "print('\\nSUMMARY:', 'OK' if all_ok else 'FAIL')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}